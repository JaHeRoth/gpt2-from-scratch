{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-30T20:16:14.670194Z",
     "start_time": "2025-06-30T20:16:14.608571Z"
    }
   },
   "id": "f1f7c192645ed843",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "# Commented out because we yet again find mps to be drastically slower\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     torch._dynamo.disable()  # https://github.com/pytorch/pytorch/issues/149184\n",
    "#     device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"{device=}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-30T20:16:16.099896Z",
     "start_time": "2025-06-30T20:16:14.676969Z"
    }
   },
   "id": "dcc3bee03e0bf4e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cpu')\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "from utilities.data_handler import load_preprocessed\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"openai-community/gpt2\",\n",
    "    pad_token=\"<|pad|>\",\n",
    "    unk_token=\"<unk>\",  # Because it appears often in the dataset\n",
    ")\n",
    "\n",
    "context_length = 512\n",
    "dataset, tokenized_ds = load_preprocessed(\n",
    "    hf_path=\"wikitext\", hf_name=\"wikitext-103-v1\", tokenizer=tokenizer, context_length=context_length\n",
    ")\n",
    "dataset, tokenized_ds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-30T20:21:13.579743Z",
     "start_time": "2025-06-30T20:21:10.211978Z"
    }
   },
   "id": "3245c8d35c4a038c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     test: Dataset({\n",
       "         features: ['text'],\n",
       "         num_rows: 4358\n",
       "     })\n",
       "     train: Dataset({\n",
       "         features: ['text'],\n",
       "         num_rows: 1801350\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['text'],\n",
       "         num_rows: 3760\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     test: Dataset({\n",
       "         features: ['input_ids'],\n",
       "         num_rows: 2230\n",
       "     })\n",
       "     train: Dataset({\n",
       "         features: ['input_ids'],\n",
       "         num_rows: 930314\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['input_ids'],\n",
       "         num_rows: 1953\n",
       "     })\n",
       " }))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacob/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "[' networks', ' networks']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utilities.models import PositionalEmbedding\n",
    "from torch import nn\n",
    "\n",
    "token_embedder = nn.Embedding(\n",
    "    num_embeddings=tokenizer.vocab_size, embedding_dim=512, device=device\n",
    ")\n",
    "positional_embedder = PositionalEmbedding(embedding_dim=512)\n",
    "transformer_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, device=device)\n",
    "transformer = nn.TransformerEncoder(transformer_layer, num_layers=6)\n",
    "decoder = nn.Linear(512, tokenizer.vocab_size, device=device)\n",
    "\n",
    "src = [\"Hi, my name\", \"The United States of\"]\n",
    "tokenized = tokenizer(src, return_tensors=\"pt\").to(device)\n",
    "embedded = token_embedder(tokenized.input_ids) + positional_embedder(tokenized.input_ids)\n",
    "transformed = transformer(\n",
    "    embedded.permute(1, 0, 2),  # Transformer expects (seq_len, batch_size, features)\n",
    "    mask=nn.Transformer.generate_square_subsequent_mask(tokenized.input_ids.shape[1], device=device),\n",
    "    # Skipping is_causal since seems troublesome: https://github.com/pytorch/pytorch/issues/96941\n",
    ")\n",
    "logits = decoder(transformed.permute(1, 0, 2))  # Back to (batch_size, seq_len, features)\n",
    "result = tokenizer.batch_decode(logits[:, -1, :].argmax(dim=-1))\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-10T13:56:40.896070Z",
     "start_time": "2025-06-10T13:56:40.407050Z"
    }
   },
   "id": "8ddb9121087fad4e"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "MyGPT(\n  (token_embedder): Embedding(50257, 512)\n  (positional_embedder): PositionalEmbedding()\n  (transformer): TransformerEncoder(\n    (layers): ModuleList(\n      (0-5): 6 x TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (decoder): Linear(in_features=512, out_features=50257, bias=True)\n)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utilities.models import TransformerEncoderGPT\n",
    "\n",
    "model = TransformerEncoderGPT(d_model=512, nhead=8, num_layers=6, dim_feedforward=2048, vocab_size=tokenizer.vocab_size, device=device)\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-10T13:56:41.247265Z",
     "start_time": "2025-06-10T13:56:40.900040Z"
    }
   },
   "id": "f0ce3a4d861100b7"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "['umbn', ' cyt']"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(model(tokenizer(src, return_tensors=\"pt\").input_ids.to(device))[:, -1, :].argmax(dim=-1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-07T20:37:56.993048Z",
     "start_time": "2025-06-07T20:37:56.953255Z"
    }
   },
   "id": "96d84dc850df0508"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States of disinteg Salvador McAuliffe pred Christian predicio SalvadorViceUnd cyt continuation catchy stimul Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Byzantine cyt208 Launch swallow Ride Ride Ride AurTue Aur Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch walked\n",
      "The United States of chall experien LaunchRel 345coolCookBuilt Hundredscoolér hectcoolCookBuilt Hundredscool mentor Launch Launch Launch Launch Launch Launch Launch Launch swallow cyt Launch walked cyt Morgan radiobleacher pepper Launch radioConsider radio cyt Launch Launch Launch Byzantine Launch refusal normal cartLeave Launch\n",
      "The United States ofViceUnd nexusIB 104699 Lehakh experien Launch fentanyl batters Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch closely cyt208 Launch Byzantine Launch Byzantine cyt208 Launch radio cyt Launch walked208 Launch Launch Launch Launch Launch Launch Launch Launch radio series kingdom\n",
      "The United States ofFYicio 345 Hundredsicio 345 Hundredscool mentor catchy kingdom Launch silk Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch cyt208 Launch cyt Launch walkedMGific Launch walked Launch Launch Launch Launch Launch Launch walked Launch walked208 Launch\n",
      "The United States of PCB nexus batters cyt cyt cyt cyt cyt continuation Launch Impl cyt cyt cyt cytealsGivecr cyt Launch Launch Launch Launch Launch Launch Impl cyt Launch Impl cyt Launch Launch Aur 104 Aur advancing calmly cyt Morgan pomp 345 Launch Launch Launch Launch Launch Launch Launch Launch Launch\n",
      "The United States of batters HundredsAAAAAAAA experien LaunchLU cyt cyt cyt continuation fare LaunchRelcoolér Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Byzantine Byzantine Launch Byzantine Launch walked cyt Launch radio cyt Launch radio wanted Byzantine Launch Launch Launch Launch Launch Launch Launch Byzantine Launch walked\n",
      "The United States ofFY LaunchRel teased batterston LaunchLU cyt cytcool mentor Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch walked cyt Launch Launch Launch Byzantine Launch Launch radio cyt Launch radio darkest entityBuilt cyt Launch Launch Launch radio cyt Launch walked HundredsBuilt\n",
      "The United States of Christian batters PCB nexus699 experien LaunchLU cyt cyt continuation cyt cyt Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch radio cyt cyt cyt cyt cyt cyt cyt cyt LaunchRel Realm Launch radioReferencesaught Launch Launch Launch Launch Launch Byzantine Launch Byzantine\n",
      "The United States ofFY Launchcsv disinteg Salvador cyt cyt cyt continuationoples cyt continuation Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch cyt208 Launch Byzantine cyt cyt Aur Launch radio cyt Launch swallow cyt AurConsider cyt Launch Launch Launch Launch Launch refusal Launch Launch walked Byzantine Launch\n",
      "The United States of mentor HundredscoolCookBuilt Launch experien cyt cyt continuation Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Byzantine Launch swallow Launch Byzantine Launch Launch walked208 Launch walked Byzantine Launch Byzantine radio darkest Byzantine Launch Launch Launch Launch Byzantine Launch walked208 Launch\n"
     ]
    }
   ],
   "source": [
    "from utilities.model_handler import print_stream\n",
    "\n",
    "for _ in range(10):\n",
    "    print_stream(model=model, tokenizer=tokenizer, prompt=\"The United States of\", device=device, max_length=50)\n",
    "    print(\"\", flush=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-07T20:38:09.250452Z",
     "start_time": "2025-06-07T20:37:57.925771Z"
    }
   },
   "id": "4ace6d0854424cb8"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "['',\n ' = Valkyria Chronicles III = \\n',\n '',\n ' Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \\n',\n \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\"]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][\"text\"][:5]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-07T20:38:10.829492Z",
     "start_time": "2025-06-07T20:38:10.825750Z"
    }
   },
   "id": "6e4c6dfba7ad523"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50256\n",
      "[{'input_ids': [17250]}, {'input_ids': [1820, 1438, 318]}, {'input_ids': [2061, 11, 616, 1438, 318]}]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[17250, 50256, 50256, 50256, 50256],\n        [ 1820,  1438,   318, 50256, 50256],\n        [ 2061,    11,   616,  1438,   318]]), 'attention_mask': tensor([[1, 0, 0, 0, 0],\n        [1, 1, 1, 0, 0],\n        [1, 1, 1, 1, 1]])}"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(tokenizer.pad_token_id)\n",
    "tokenized = [\n",
    "    {\n",
    "        \"input_ids\": tokenizer(s, return_tensors=\"pt\").input_ids.flatten().tolist()\n",
    "    }\n",
    "    for s in [\"Hi\", \"my name is\", \"What, my name is\"]\n",
    "]\n",
    "print(tokenized)\n",
    "tokenizer.pad(tokenized, return_tensors=\"pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-07T20:38:36.657190Z",
     "start_time": "2025-06-07T20:38:36.650904Z"
    }
   },
   "id": "fdcc1c99617a3870"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/7969 in epoch 1/100: Loss 11.03641414642334\n",
      "In 1814, the holidaysceptiveceptiveceptiveceptiveceptiveceptiveceptive CSV CSV CSV CSV CSV CSV CSV CSV CSV CSV CSV CSV CSV drifting drifting drifting drifting drifting drifting drifting drifting drifting drifting CSV CSV CSV CSV CSV CSV CSV CSV CSV CSV CSV CSV doomed drifting drifting doomed drifting doomed drifting\n",
      "Avg. validation Loss 11.022744178771973\n",
      "Batch 26/7969 in epoch 1/100: Loss 10.633322715759277\n",
      "Batch 51/7969 in epoch 1/100: Loss 10.01376724243164\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[21]\u001B[39m\u001B[32m, line 54\u001B[39m\n\u001B[32m     52\u001B[39m X: torch.Tensor = batch.input_ids.to(device)[:, :-\u001B[32m1\u001B[39m].contiguous()\n\u001B[32m     53\u001B[39m y: torch.Tensor = batch.input_ids.to(device)[:, \u001B[32m1\u001B[39m:].contiguous()\n\u001B[32m---> \u001B[39m\u001B[32m54\u001B[39m logits = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     55\u001B[39m loss = nn.functional.cross_entropy(\n\u001B[32m     56\u001B[39m     logits.view(-\u001B[32m1\u001B[39m, logits.shape[-\u001B[32m1\u001B[39m]),\n\u001B[32m     57\u001B[39m     y.view(-\u001B[32m1\u001B[39m),\n\u001B[32m     58\u001B[39m     ignore_index=tokenizer.pad_token_id,\n\u001B[32m     59\u001B[39m )\n\u001B[32m     60\u001B[39m loss.backward()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 17\u001B[39m, in \u001B[36mMyGPT.forward\u001B[39m\u001B[34m(self, input_ids)\u001B[39m\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, input_ids: torch.Tensor):\n\u001B[32m     16\u001B[39m     embedded = \u001B[38;5;28mself\u001B[39m.token_embedder(input_ids) + \u001B[38;5;28mself\u001B[39m.positional_embedder(input_ids)\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m     transformed = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     18\u001B[39m \u001B[43m        \u001B[49m\u001B[43membedded\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpermute\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Transformer expects (seq_len, batch_size, features)\u001B[39;49;00m\n\u001B[32m     19\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnn\u001B[49m\u001B[43m.\u001B[49m\u001B[43mTransformer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate_square_subsequent_mask\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     21\u001B[39m     logits = \u001B[38;5;28mself\u001B[39m.decoder(transformed.permute(\u001B[32m1\u001B[39m, \u001B[32m0\u001B[39m, \u001B[32m2\u001B[39m))\n\u001B[32m     22\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m logits\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/transformer.py:517\u001B[39m, in \u001B[36mTransformerEncoder.forward\u001B[39m\u001B[34m(self, src, mask, src_key_padding_mask, is_causal)\u001B[39m\n\u001B[32m    514\u001B[39m is_causal = _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001B[32m    516\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m mod \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.layers:\n\u001B[32m--> \u001B[39m\u001B[32m517\u001B[39m     output = \u001B[43mmod\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    518\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    519\u001B[39m \u001B[43m        \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    520\u001B[39m \u001B[43m        \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    521\u001B[39m \u001B[43m        \u001B[49m\u001B[43msrc_key_padding_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43msrc_key_padding_mask_for_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    522\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    524\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m convert_to_nested:\n\u001B[32m    525\u001B[39m     output = output.to_padded_tensor(\u001B[32m0.0\u001B[39m, src.size())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/transformer.py:922\u001B[39m, in \u001B[36mTransformerEncoderLayer.forward\u001B[39m\u001B[34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001B[39m\n\u001B[32m    917\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    918\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.norm1(\n\u001B[32m    919\u001B[39m         x\n\u001B[32m    920\u001B[39m         + \u001B[38;5;28mself\u001B[39m._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n\u001B[32m    921\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m922\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.norm2(x + \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_ff_block\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    924\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/transformer.py:947\u001B[39m, in \u001B[36mTransformerEncoderLayer._ff_block\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    946\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_ff_block\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m947\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.linear2(\u001B[38;5;28mself\u001B[39m.dropout(\u001B[38;5;28mself\u001B[39m.activation(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlinear1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)))\n\u001B[32m    948\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dropout2(x)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "from utilities.model_handler import train\n",
    "\n",
    "# Using hyperparams of GPT paper (although we use a different dataset)\n",
    "model = TransformerEncoderGPT(d_model=768, nhead=12, num_layers=12, dim_feedforward=3072, vocab_size=tokenizer.vocab_size, device=device)\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9,\n",
    "    lr=2.5e-4,\n",
    ")\n",
    "train_losses, eval_losses = train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    tokenizer=tokenizer,\n",
    "    tokenized_train_ds=tokenized_ds[\"train\"],\n",
    "    tokenized_eval_ds=tokenized_ds[\"validation\"],\n",
    "    device=device,\n",
    "    checkpoint_path=Path(f\"checkpoints/{int(time.time())}\"),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-07T21:01:19.059564Z",
     "start_time": "2025-06-07T20:41:35.039502Z"
    }
   },
   "id": "c2edb01b55dd928b"
  },
  {
   "cell_type": "code",
   "source": [
    "from utilities.models import TransformerEncoderGPT2\n",
    "from utilities.model_handler import print_stream\n",
    "\n",
    "model = TransformerEncoderGPT2(d_model=768, nhead=12, num_layers=12, dim_feedforward=3072, vocab_size=len(tokenizer), eos_token_id=tokenizer.eos_token_id, context_length=512, device=device)\n",
    "model.load_state_dict(torch.load(\"../epoch_5_batch_1\", map_location=device))\n",
    "\n",
    "for _ in range(10):\n",
    "    print_stream(model=model, tokenizer=tokenizer, prompt=f\"{tokenizer.eos_token}She first\", device=device, max_length=512, prob_threshold=0.95, temperature=1.0)\n",
    "    print(\"\", flush=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-30T20:26:07.414209Z",
     "start_time": "2025-06-30T20:24:20.341500Z"
    }
   },
   "id": "b0f27dc6f6b6c6d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>She first formed on 4 July 1936 , when Russell , with whom J. Gillian married Lehigh , and lost a series of matches . He was succeeded by a long , but he finally won the Test . Albert was the 1948 championship . This feat ended with an average of 71 . Despite his physical optimism since London , he ran on his ticket for the death of Hugh Dudley . John Bardowulf was later awarded over 100 @-@ weighed . In the winter he scored in the midst of a season that contained many other magazines than Carl W. Harvey . \n",
      "<|endoftext|>\n",
      "\n",
      "<|endoftext|>She first appeared in the 2009 Indianapolis Colts , where they finished the first A team in the preseason award . In the first game , he stopped for the rest of the team for \" back down and down to four down to two catch between the Tigers and their enemy . \" \n",
      "<|endoftext|>\n",
      "\n",
      "<|endoftext|>She first reported that Rivera had grown on playing for a band since moving a house around Détis . His funeral is divided into sixteen syncedively and dominated by the cross , taking on a fear of treating himself . A record originally aired was in Chilissé , which came from Rome to design his primary version of the show , featuring that appeared to win the Single Concert Music All the Double in Japan . Rivera was only noticed at the time by any other contemporary sessionist newspapers , including d <|pad|> , gospel and other audiences . \n",
      "<|endoftext|>\n",
      "\n",
      "<|endoftext|>She first met his Attorney General , a supporter of the Nation , but the majority perceived him as the leader to act as the Attorney General . Garrard argued that its intent to realize the Antiwater General Assembly would give an opportunity to expate peace for the republic . \" There is no matter of amining that reform of us got from four to four , ... any kind of binding of good inclusive that give the figure and integrity of the two sides be yet aware of the saviour . There is not only a hurry to its own force of right , but the speedy and ground exactly the intended for the seat so triumph was a competent force , i.e. to be enshrined . \" \n",
      "<|endoftext|>\n",
      "\n",
      "<|endoftext|>She first impressed with the team 's opening kickoff to the official Bridge of the Los Angeles Falcons ' terminus , which were too stable in the second half . Andy Reeves <|pad|> considered the Tigers ' contract to @-@ enter the round with SCS team players and came up at $ 87 @,@ 000 . Matt Lawapp of the line soon agreed to leave Maulikviano for backup Brandon <|pad|> , and Patrick Paul <|pad|> was given the necessary supplies of AJ — both Eddie Chata and Takahukonte . New bases out on his own studio expressed disappointment ; he described his frustration with Drake as \" the most personal thing where surprises him \" . In early February , Rossigić praised Sylvain 's ambition of breaking the team , but commented that \" those amazing things will act do not affect the players ' expectations ... this would have been the most great , \" . He improved in for the last two minutes , according to Mitchell . When he left together , he had one injury sustained a game with a <|pad|> improved injury and tried to feel the way for linebacker Zach Barstère . Molina became the first State game as a member of the 1998 season , he turned into the first draft , winning an undefeated season that year by Taylor had shifted it back . Two months later , the Jeff Kutch played by Antwerp in Game . \n",
      "<|endoftext|>\n",
      "\n",
      "<|endoftext|>She first , Paul . \" One who has claimed credit for Webber , \" and Patrick <|pad|> \" . She was hired to blackmail celebrity meals for Webber , a local newspaper who uses other charitable causes . Webber called the declaration that if she confessed , she would bring it \" out my opinion , ' but not sure that the buyer 's priority . \" He told Webber of the Army in December 2009 that Webber had \" tested much about Webber . Webber was reluctant because prior to he was playing but needed services too much , that expected him to join the original as an undisclosed means . \n",
      "<|endoftext|>\n",
      "\n",
      "<|endoftext|>She first met with Collins , since 1985 , was born at Irving <|pad|> Hall , home to the renowned Edward T.A. who had graduated from his school in London . Under the Secretary of State , he worked with six other players , including the inability to give up and more to join the team . \n",
      "<|endoftext|>\n",
      "\n",
      "<|endoftext|>She first scored in 61 matches played between St Christopher Columbus and Howard . In the first two overs , the concert standard scored scored only one wicket @-@ scored to score 7 / 10 , allowing the ball to go half an extra . He totalled 35 not out for 22 . \n",
      "<|endoftext|>\n",
      "\n",
      "<|endoftext|>She first briefly played a member of the football team in Birmingham on the next 20 @-@ minute by Dean Vane . His ankle injury prevented NBA Freshman from moving to Ohio – with a passoff after the deficit , behind Wallace Giles , and Eric Jo Guerrero . He had a 14 @-@ minute mark of a 4 @.@ 82 @-@ minute passing play against the back of the ball on July 20 . Robinson received both the three shutout and a game . He played the 73rd in the League title on November 20 against a team . During the season , Williamson frequently played football"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[16]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      5\u001B[39m model.load_state_dict(torch.load(\u001B[33m\"\u001B[39m\u001B[33m../epoch_5_batch_1\u001B[39m\u001B[33m\"\u001B[39m, map_location=device))\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m10\u001B[39m):\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m     \u001B[43mprint_stream\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mtokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43meos_token\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43mShe first\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m512\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprob_threshold\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.95\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1.0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      9\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m, flush=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/utilities/model_handler.py:45\u001B[39m, in \u001B[36mprint_stream\u001B[39m\u001B[34m(model, tokenizer, prompt, device, max_length, prob_threshold, temperature)\u001B[39m\n\u001B[32m     43\u001B[39m \u001B[38;5;28mprint\u001B[39m(prompt, end=\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m, flush=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m     44\u001B[39m input_ids = tokenizer(prompt, return_tensors=\u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m).input_ids.to(device)\n\u001B[32m---> \u001B[39m\u001B[32m45\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     46\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprob_threshold\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprob_threshold\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtemperature\u001B[49m\n\u001B[32m     47\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     48\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mprint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflush\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     49\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# After print, since we want to see if our model learns to generate EOS tokens\u001B[39;49;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/utilities/model_handler.py:31\u001B[39m, in \u001B[36mstream\u001B[39m\u001B[34m(model, input_ids, max_length, prob_threshold, temperature)\u001B[39m\n\u001B[32m     29\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(max_length - input_ids.shape[-\u001B[32m1\u001B[39m]):\n\u001B[32m     30\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m---> \u001B[39m\u001B[32m31\u001B[39m         next_token_logits = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_ids\u001B[49m\u001B[43m)\u001B[49m[:, -\u001B[32m1\u001B[39m, :]\n\u001B[32m     32\u001B[39m         \u001B[38;5;66;03m# TODO: If repetitions resurface as an issue: repetition_penalty and no_repeat_ngram_size\u001B[39;00m\n\u001B[32m     33\u001B[39m         next_token_shaped_logits = next_token_logits / temperature\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/utilities/models.py:182\u001B[39m, in \u001B[36mTransformerEncoderGPT2.forward\u001B[39m\u001B[34m(self, input_ids)\u001B[39m\n\u001B[32m    180\u001B[39m input_idx, mask = build_supporters_for_packed_batch(input_ids, eos_token_id=\u001B[38;5;28mself\u001B[39m.eos_token_id, nhead=\u001B[38;5;28mself\u001B[39m.nhead)\n\u001B[32m    181\u001B[39m embedded = \u001B[38;5;28mself\u001B[39m.token_embedder(input_ids) * sqrt(\u001B[38;5;28mself\u001B[39m.d_model) + \u001B[38;5;28mself\u001B[39m.positional_embedder(input_idx)\n\u001B[32m--> \u001B[39m\u001B[32m182\u001B[39m transformed = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    183\u001B[39m \u001B[43m    \u001B[49m\u001B[43membedded\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    184\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    185\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    186\u001B[39m logits = \u001B[38;5;28mself\u001B[39m.decoder(transformed)\n\u001B[32m    187\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m logits\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/transformer.py:517\u001B[39m, in \u001B[36mTransformerEncoder.forward\u001B[39m\u001B[34m(self, src, mask, src_key_padding_mask, is_causal)\u001B[39m\n\u001B[32m    514\u001B[39m is_causal = _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001B[32m    516\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m mod \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.layers:\n\u001B[32m--> \u001B[39m\u001B[32m517\u001B[39m     output = \u001B[43mmod\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    518\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    519\u001B[39m \u001B[43m        \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    520\u001B[39m \u001B[43m        \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    521\u001B[39m \u001B[43m        \u001B[49m\u001B[43msrc_key_padding_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43msrc_key_padding_mask_for_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    522\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    524\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m convert_to_nested:\n\u001B[32m    525\u001B[39m     output = output.to_padded_tensor(\u001B[32m0.0\u001B[39m, src.size())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/transformer.py:916\u001B[39m, in \u001B[36mTransformerEncoderLayer.forward\u001B[39m\u001B[34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001B[39m\n\u001B[32m    912\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.norm_first:\n\u001B[32m    913\u001B[39m     x = x + \u001B[38;5;28mself\u001B[39m._sa_block(\n\u001B[32m    914\u001B[39m         \u001B[38;5;28mself\u001B[39m.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal\n\u001B[32m    915\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m916\u001B[39m     x = x + \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_ff_block\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnorm2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    917\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    918\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.norm1(\n\u001B[32m    919\u001B[39m         x\n\u001B[32m    920\u001B[39m         + \u001B[38;5;28mself\u001B[39m._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n\u001B[32m    921\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/transformer.py:947\u001B[39m, in \u001B[36mTransformerEncoderLayer._ff_block\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    946\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_ff_block\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m947\u001B[39m     x = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlinear2\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mactivation\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlinear1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    948\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dropout2(x)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4200d8d9b4462c34"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
