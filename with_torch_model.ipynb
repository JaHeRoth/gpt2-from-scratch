{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-11T06:58:02.416394Z",
     "start_time": "2025-06-11T06:58:02.353560Z"
    }
   },
   "id": "f1f7c192645ed843"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "# Commented out because we yet again find mps to be drastically slower\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     torch._dynamo.disable()  # https://github.com/pytorch/pytorch/issues/149184\n",
    "#     device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"{device=}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-10T13:56:40.263761Z",
     "start_time": "2025-06-10T13:56:31.326493Z"
    }
   },
   "id": "dcc3bee03e0bf4e3"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    test: Dataset({\n        features: ['input_ids'],\n        num_rows: 1184\n    })\n    train: Dataset({\n        features: ['input_ids'],\n        num_rows: 509994\n    })\n    validation: Dataset({\n        features: ['input_ids'],\n        num_rows: 1037\n    })\n})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from data_handler import load_preprocessed\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# TODO: Use BooksCorpus dataset and context_length=512\n",
    "context_length = 128\n",
    "dataset, tokenized_ds = load_preprocessed(\n",
    "    hf_path=\"wikitext\", hf_name=\"wikitext-103-v1\", tokenizer=tokenizer, context_length=context_length\n",
    ")\n",
    "dataset, tokenized_ds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-10T13:56:40.406349Z",
     "start_time": "2025-06-10T13:56:40.265184Z"
    }
   },
   "id": "3245c8d35c4a038c"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacob/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "[' networks', ' networks']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import PositionalEmbedding\n",
    "from torch import nn\n",
    "\n",
    "token_embedder = nn.Embedding(\n",
    "    num_embeddings=tokenizer.vocab_size, embedding_dim=512, device=device\n",
    ")\n",
    "positional_embedder = PositionalEmbedding(embedding_dim=512)\n",
    "transformer_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, device=device)\n",
    "transformer = nn.TransformerEncoder(transformer_layer, num_layers=6)\n",
    "decoder = nn.Linear(512, tokenizer.vocab_size, device=device)\n",
    "\n",
    "src = [\"Hi, my name\", \"The United States of\"]\n",
    "tokenized = tokenizer(src, return_tensors=\"pt\").to(device)\n",
    "embedded = token_embedder(tokenized.input_ids) + positional_embedder(tokenized.input_ids)\n",
    "transformed = transformer(\n",
    "    embedded.permute(1, 0, 2),  # Transformer expects (seq_len, batch_size, features)\n",
    "    mask=nn.Transformer.generate_square_subsequent_mask(tokenized.input_ids.shape[1], device=device),\n",
    "    # Skipping is_causal since seems troublesome: https://github.com/pytorch/pytorch/issues/96941\n",
    ")\n",
    "logits = decoder(transformed.permute(1, 0, 2))  # Back to (batch_size, seq_len, features)\n",
    "result = tokenizer.batch_decode(logits[:, -1, :].argmax(dim=-1))\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-10T13:56:40.896070Z",
     "start_time": "2025-06-10T13:56:40.407050Z"
    }
   },
   "id": "8ddb9121087fad4e"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "MyGPT(\n  (token_embedder): Embedding(50257, 512)\n  (positional_embedder): PositionalEmbedding()\n  (transformer): TransformerEncoder(\n    (layers): ModuleList(\n      (0-5): 6 x TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (decoder): Linear(in_features=512, out_features=50257, bias=True)\n)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import MyGPT\n",
    "\n",
    "model = MyGPT(d_model=512, nhead=8, num_layers=6, dim_feedforward=2048, vocab_size=tokenizer.vocab_size, device=device)\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-10T13:56:41.247265Z",
     "start_time": "2025-06-10T13:56:40.900040Z"
    }
   },
   "id": "f0ce3a4d861100b7"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "['umbn', ' cyt']"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(model(tokenizer(src, return_tensors=\"pt\").input_ids.to(device))[:, -1, :].argmax(dim=-1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-07T20:37:56.993048Z",
     "start_time": "2025-06-07T20:37:56.953255Z"
    }
   },
   "id": "96d84dc850df0508"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States of disinteg Salvador McAuliffe pred Christian predicio SalvadorViceUnd cyt continuation catchy stimul Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Byzantine cyt208 Launch swallow Ride Ride Ride AurTue Aur Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch walked\n",
      "The United States of chall experien LaunchRel 345coolCookBuilt Hundredscoolér hectcoolCookBuilt Hundredscool mentor Launch Launch Launch Launch Launch Launch Launch Launch swallow cyt Launch walked cyt Morgan radiobleacher pepper Launch radioConsider radio cyt Launch Launch Launch Byzantine Launch refusal normal cartLeave Launch\n",
      "The United States ofViceUnd nexusIB 104699 Lehakh experien Launch fentanyl batters Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch closely cyt208 Launch Byzantine Launch Byzantine cyt208 Launch radio cyt Launch walked208 Launch Launch Launch Launch Launch Launch Launch Launch radio series kingdom\n",
      "The United States ofFYicio 345 Hundredsicio 345 Hundredscool mentor catchy kingdom Launch silk Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch cyt208 Launch cyt Launch walkedMGific Launch walked Launch Launch Launch Launch Launch Launch walked Launch walked208 Launch\n",
      "The United States of PCB nexus batters cyt cyt cyt cyt cyt continuation Launch Impl cyt cyt cyt cytealsGivecr cyt Launch Launch Launch Launch Launch Launch Impl cyt Launch Impl cyt Launch Launch Aur 104 Aur advancing calmly cyt Morgan pomp 345 Launch Launch Launch Launch Launch Launch Launch Launch Launch\n",
      "The United States of batters HundredsAAAAAAAA experien LaunchLU cyt cyt cyt continuation fare LaunchRelcoolér Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Byzantine Byzantine Launch Byzantine Launch walked cyt Launch radio cyt Launch radio wanted Byzantine Launch Launch Launch Launch Launch Launch Launch Byzantine Launch walked\n",
      "The United States ofFY LaunchRel teased batterston LaunchLU cyt cytcool mentor Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch walked cyt Launch Launch Launch Byzantine Launch Launch radio cyt Launch radio darkest entityBuilt cyt Launch Launch Launch radio cyt Launch walked HundredsBuilt\n",
      "The United States of Christian batters PCB nexus699 experien LaunchLU cyt cyt continuation cyt cyt Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch radio cyt cyt cyt cyt cyt cyt cyt cyt LaunchRel Realm Launch radioReferencesaught Launch Launch Launch Launch Launch Byzantine Launch Byzantine\n",
      "The United States ofFY Launchcsv disinteg Salvador cyt cyt cyt continuationoples cyt continuation Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch cyt208 Launch Byzantine cyt cyt Aur Launch radio cyt Launch swallow cyt AurConsider cyt Launch Launch Launch Launch Launch refusal Launch Launch walked Byzantine Launch\n",
      "The United States of mentor HundredscoolCookBuilt Launch experien cyt cyt continuation Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Launch Byzantine Launch swallow Launch Byzantine Launch Launch walked208 Launch walked Byzantine Launch Byzantine radio darkest Byzantine Launch Launch Launch Launch Byzantine Launch walked208 Launch\n"
     ]
    }
   ],
   "source": [
    "from model_handler import print_stream\n",
    "\n",
    "for _ in range(10):\n",
    "    print_stream(model=model, tokenizer=tokenizer, prompt=\"The United States of\", device=device, max_length=50)\n",
    "    print(\"\", flush=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-07T20:38:09.250452Z",
     "start_time": "2025-06-07T20:37:57.925771Z"
    }
   },
   "id": "4ace6d0854424cb8"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "['',\n ' = Valkyria Chronicles III = \\n',\n '',\n ' Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \\n',\n \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\"]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][\"text\"][:5]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-07T20:38:10.829492Z",
     "start_time": "2025-06-07T20:38:10.825750Z"
    }
   },
   "id": "6e4c6dfba7ad523"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50256\n",
      "[{'input_ids': [17250]}, {'input_ids': [1820, 1438, 318]}, {'input_ids': [2061, 11, 616, 1438, 318]}]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[17250, 50256, 50256, 50256, 50256],\n        [ 1820,  1438,   318, 50256, 50256],\n        [ 2061,    11,   616,  1438,   318]]), 'attention_mask': tensor([[1, 0, 0, 0, 0],\n        [1, 1, 1, 0, 0],\n        [1, 1, 1, 1, 1]])}"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(tokenizer.pad_token_id)\n",
    "tokenized = [\n",
    "    {\n",
    "        \"input_ids\": tokenizer(s, return_tensors=\"pt\").input_ids.flatten().tolist()\n",
    "    }\n",
    "    for s in [\"Hi\", \"my name is\", \"What, my name is\"]\n",
    "]\n",
    "print(tokenized)\n",
    "tokenizer.pad(tokenized, return_tensors=\"pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-07T20:38:36.657190Z",
     "start_time": "2025-06-07T20:38:36.650904Z"
    }
   },
   "id": "fdcc1c99617a3870"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/7969 in epoch 1/100: Loss 11.03641414642334\n",
      "In 1814, the holidaysceptiveceptiveceptiveceptiveceptiveceptiveceptive CSV CSV CSV CSV CSV CSV CSV CSV CSV CSV CSV CSV CSV drifting drifting drifting drifting drifting drifting drifting drifting drifting drifting CSV CSV CSV CSV CSV CSV CSV CSV CSV CSV CSV CSV doomed drifting drifting doomed drifting doomed drifting\n",
      "Avg. validation Loss 11.022744178771973\n",
      "Batch 26/7969 in epoch 1/100: Loss 10.633322715759277\n",
      "Batch 51/7969 in epoch 1/100: Loss 10.01376724243164\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[21]\u001B[39m\u001B[32m, line 54\u001B[39m\n\u001B[32m     52\u001B[39m X: torch.Tensor = batch.input_ids.to(device)[:, :-\u001B[32m1\u001B[39m].contiguous()\n\u001B[32m     53\u001B[39m y: torch.Tensor = batch.input_ids.to(device)[:, \u001B[32m1\u001B[39m:].contiguous()\n\u001B[32m---> \u001B[39m\u001B[32m54\u001B[39m logits = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     55\u001B[39m loss = nn.functional.cross_entropy(\n\u001B[32m     56\u001B[39m     logits.view(-\u001B[32m1\u001B[39m, logits.shape[-\u001B[32m1\u001B[39m]),\n\u001B[32m     57\u001B[39m     y.view(-\u001B[32m1\u001B[39m),\n\u001B[32m     58\u001B[39m     ignore_index=tokenizer.pad_token_id,\n\u001B[32m     59\u001B[39m )\n\u001B[32m     60\u001B[39m loss.backward()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 17\u001B[39m, in \u001B[36mMyGPT.forward\u001B[39m\u001B[34m(self, input_ids)\u001B[39m\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, input_ids: torch.Tensor):\n\u001B[32m     16\u001B[39m     embedded = \u001B[38;5;28mself\u001B[39m.token_embedder(input_ids) + \u001B[38;5;28mself\u001B[39m.positional_embedder(input_ids)\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m     transformed = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     18\u001B[39m \u001B[43m        \u001B[49m\u001B[43membedded\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpermute\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Transformer expects (seq_len, batch_size, features)\u001B[39;49;00m\n\u001B[32m     19\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnn\u001B[49m\u001B[43m.\u001B[49m\u001B[43mTransformer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate_square_subsequent_mask\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     21\u001B[39m     logits = \u001B[38;5;28mself\u001B[39m.decoder(transformed.permute(\u001B[32m1\u001B[39m, \u001B[32m0\u001B[39m, \u001B[32m2\u001B[39m))\n\u001B[32m     22\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m logits\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/transformer.py:517\u001B[39m, in \u001B[36mTransformerEncoder.forward\u001B[39m\u001B[34m(self, src, mask, src_key_padding_mask, is_causal)\u001B[39m\n\u001B[32m    514\u001B[39m is_causal = _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001B[32m    516\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m mod \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.layers:\n\u001B[32m--> \u001B[39m\u001B[32m517\u001B[39m     output = \u001B[43mmod\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    518\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    519\u001B[39m \u001B[43m        \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    520\u001B[39m \u001B[43m        \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    521\u001B[39m \u001B[43m        \u001B[49m\u001B[43msrc_key_padding_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43msrc_key_padding_mask_for_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    522\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    524\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m convert_to_nested:\n\u001B[32m    525\u001B[39m     output = output.to_padded_tensor(\u001B[32m0.0\u001B[39m, src.size())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/transformer.py:922\u001B[39m, in \u001B[36mTransformerEncoderLayer.forward\u001B[39m\u001B[34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001B[39m\n\u001B[32m    917\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    918\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.norm1(\n\u001B[32m    919\u001B[39m         x\n\u001B[32m    920\u001B[39m         + \u001B[38;5;28mself\u001B[39m._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n\u001B[32m    921\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m922\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.norm2(x + \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_ff_block\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    924\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/transformer.py:947\u001B[39m, in \u001B[36mTransformerEncoderLayer._ff_block\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    946\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_ff_block\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m947\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.linear2(\u001B[38;5;28mself\u001B[39m.dropout(\u001B[38;5;28mself\u001B[39m.activation(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlinear1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)))\n\u001B[32m    948\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dropout2(x)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "from model_handler import train\n",
    "\n",
    "# All hyperparams in this cell were copied from GPT paper (although we use a different dataset)\n",
    "model = MyGPT(d_model=768, nhead=12, num_layers=12, dim_feedforward=3072, vocab_size=tokenizer.vocab_size, device=device)\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9,\n",
    "    lr=2.5e-4,\n",
    ")\n",
    "train_losses, eval_losses = train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    tokenizer=tokenizer,\n",
    "    tokenized_train_ds=tokenized_ds[\"train\"],\n",
    "    tokenized_eval_ds=tokenized_ds[\"validation\"],\n",
    "    device=device,\n",
    "    checkpoint_path=Path(f\"checkpoints/{int(time.time())}\"),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-07T21:01:19.059564Z",
     "start_time": "2025-06-07T20:41:35.039502Z"
    }
   },
   "id": "c2edb01b55dd928b"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He first met the young woman in the early 1960s , when she was still a teenager , and she became a professional in the early 1960s . She was soon joined by her sister , who became her husband 's manager and her mother . She was the\n",
      "\n",
      "He first appeared in the media in the second issue of the magazine , in October , the first issue of the magazine , and in the first issue of the magazine , she was the first to be published in the magazine . She was also the first female editor of\n",
      "\n",
      "He first appeared in the first season of the American television series The Simpsons , where he was played by John Frasier . Frasier 's role in the episode was based on the character of Homer , a former member of the show 's staff .\n",
      "\n",
      "He first met with the British Army on 9 March 1917 , and was accepted into the Royal Australian Air Force ( RAAF ) on 10 March . Posted to the Middle East in April 1915 , he was posted to No. 1 Squadron , flying Sopwith Camels\n",
      "\n",
      "He first appeared in the second season of the American television medical drama Grey 's Anatomy , where she was cast as the love interest of Dr. Leo Spaceman . The series was originally broadcast in the United States on Grey 's Anatomy , but was\n",
      "\n",
      "He first appears in the season three episode \" The Power of the Three \" , where he is a member of the resistance movement to the death of his wife . He is a member of the resistance movement , which is a group of revolutionaries . The group is led\n",
      "\n",
      "He first performed the song at the 2008 MTV Video Music Awards , where she performed \" Don 't Cry Me Argentina \" , \" Let Me Die \" , \" Let There Be Love \" , \" Let Me Ride \" and \" Let Me Die \" . The performance\n",
      "\n",
      "He first appeared in the series in the first season episode \" The Girl Who Loved Me \" , which was originally written by series creator Chris Carter , and later the show 's lead actress . The series ' first broadcast was the sixth episode of the series ,\n",
      "\n",
      "He first appeared in the film in the 1991 film The Last of the Summer Wine , which was based on the novel of the same name by John Carpenter . Carpenter was a fan of the book and was inspired to write the script . Carpenter was inspired by the novel\n",
      "\n",
      "He first appeared in the game , and the game was played in a 3D format with a 3D model . The game was released in the United States on November 14 , 1998 , and in Europe on November 15 , 1999 . The game was released in Japan\n"
     ]
    }
   ],
   "source": [
    "model = MyGPT(d_model=768, nhead=12, num_layers=12, dim_feedforward=3072, vocab_size=tokenizer.vocab_size, device=device)\n",
    "model.load_state_dict(torch.load(\"model_weights.pth\", map_location=device))\n",
    "\n",
    "for _ in range(10):\n",
    "    print_stream(model=model, tokenizer=tokenizer, prompt=\"He first\", device=device, max_length=50, prob_threshold=0.95, temperature=0.3)\n",
    "    print(\"\", flush=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-10T13:58:39.007681Z",
     "start_time": "2025-06-10T13:58:11.686826Z"
    }
   },
   "id": "b0f27dc6f6b6c6d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7a934e30b18592b4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
