{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cpu')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "# Commented out because we yet again find mps to be drastically slower\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     torch._dynamo.disable()  # https://github.com/pytorch/pytorch/issues/149184\n",
    "#     device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"{device=}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T20:22:07.431844Z",
     "start_time": "2025-06-06T20:22:06.539892Z"
    }
   },
   "id": "f1f7c192645ed843"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T20:22:15.239512Z",
     "start_time": "2025-06-06T20:22:07.431651Z"
    }
   },
   "id": "dcc3bee03e0bf4e3"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/1801350 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1a26d0e2d3948318da252d66616425a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 20\u001B[39m\n\u001B[32m      5\u001B[39m     outputs = tokenizer(\n\u001B[32m      6\u001B[39m         batch[\u001B[33m\"\u001B[39m\u001B[33mtext\u001B[39m\u001B[33m\"\u001B[39m],\n\u001B[32m      7\u001B[39m         truncation=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     10\u001B[39m         return_length=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m     11\u001B[39m     )\n\u001B[32m     12\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[32m     13\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33minput_ids\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m     14\u001B[39m             input_ids\n\u001B[32m   (...)\u001B[39m\u001B[32m     17\u001B[39m         ]\n\u001B[32m     18\u001B[39m     }\n\u001B[32m---> \u001B[39m\u001B[32m20\u001B[39m tokenized_ds = \u001B[43mdataset\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     21\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtokenize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatched\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mremove_columns\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtrain\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcolumn_names\u001B[49m\n\u001B[32m     22\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     23\u001B[39m tokenized_ds.save_to_disk(\u001B[33m\"\u001B[39m\u001B[33mtokenized-wiki-ds.hf\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     24\u001B[39m tokenized_ds\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/datasets/dataset_dict.py:941\u001B[39m, in \u001B[36mDatasetDict.map\u001B[39m\u001B[34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001B[39m\n\u001B[32m    938\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m with_split:\n\u001B[32m    939\u001B[39m     function = bind(function, split)\n\u001B[32m--> \u001B[39m\u001B[32m941\u001B[39m dataset_dict[split] = \u001B[43mdataset\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    942\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfunction\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfunction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    943\u001B[39m \u001B[43m    \u001B[49m\u001B[43mwith_indices\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwith_indices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    944\u001B[39m \u001B[43m    \u001B[49m\u001B[43mwith_rank\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwith_rank\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    945\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_columns\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_columns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    946\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatched\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatched\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    947\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    948\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdrop_last_batch\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdrop_last_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    949\u001B[39m \u001B[43m    \u001B[49m\u001B[43mremove_columns\u001B[49m\u001B[43m=\u001B[49m\u001B[43mremove_columns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    950\u001B[39m \u001B[43m    \u001B[49m\u001B[43mkeep_in_memory\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkeep_in_memory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    951\u001B[39m \u001B[43m    \u001B[49m\u001B[43mload_from_cache_file\u001B[49m\u001B[43m=\u001B[49m\u001B[43mload_from_cache_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    952\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_file_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_file_names\u001B[49m\u001B[43m[\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    953\u001B[39m \u001B[43m    \u001B[49m\u001B[43mwriter_batch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mwriter_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    954\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    955\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdisable_nullable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdisable_nullable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    956\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfn_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfn_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    957\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    958\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdesc\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdesc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    959\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    961\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m with_split:\n\u001B[32m    962\u001B[39m     function = function.func\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/datasets/arrow_dataset.py:557\u001B[39m, in \u001B[36mtransmit_format.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    550\u001B[39m self_format = {\n\u001B[32m    551\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtype\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m._format_type,\n\u001B[32m    552\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mformat_kwargs\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m._format_kwargs,\n\u001B[32m    553\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mcolumns\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m._format_columns,\n\u001B[32m    554\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33moutput_all_columns\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m._output_all_columns,\n\u001B[32m    555\u001B[39m }\n\u001B[32m    556\u001B[39m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m557\u001B[39m out: Union[\u001B[33m\"\u001B[39m\u001B[33mDataset\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mDatasetDict\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    558\u001B[39m datasets: \u001B[38;5;28mlist\u001B[39m[\u001B[33m\"\u001B[39m\u001B[33mDataset\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28mlist\u001B[39m(out.values()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[32m    559\u001B[39m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/datasets/arrow_dataset.py:3074\u001B[39m, in \u001B[36mDataset.map\u001B[39m\u001B[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[39m\n\u001B[32m   3068\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m transformed_dataset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   3069\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m hf_tqdm(\n\u001B[32m   3070\u001B[39m         unit=\u001B[33m\"\u001B[39m\u001B[33m examples\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   3071\u001B[39m         total=pbar_total,\n\u001B[32m   3072\u001B[39m         desc=desc \u001B[38;5;129;01mor\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mMap\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   3073\u001B[39m     ) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[32m-> \u001B[39m\u001B[32m3074\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrank\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdone\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontent\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mDataset\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_map_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mdataset_kwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   3075\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdone\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   3076\u001B[39m \u001B[43m                \u001B[49m\u001B[43mshards_done\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/datasets/arrow_dataset.py:3516\u001B[39m, in \u001B[36mDataset._map_single\u001B[39m\u001B[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001B[39m\n\u001B[32m   3514\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   3515\u001B[39m     _time = time.time()\n\u001B[32m-> \u001B[39m\u001B[32m3516\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miter_outputs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshard_iterable\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   3517\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnum_examples_in_batch\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3518\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mupdate_data\u001B[49m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/datasets/arrow_dataset.py:3466\u001B[39m, in \u001B[36mDataset._map_single.<locals>.iter_outputs\u001B[39m\u001B[34m(shard_iterable)\u001B[39m\n\u001B[32m   3464\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   3465\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m i, example \u001B[38;5;129;01min\u001B[39;00m shard_iterable:\n\u001B[32m-> \u001B[39m\u001B[32m3466\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m i, \u001B[43mapply_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moffset\u001B[49m\u001B[43m=\u001B[49m\u001B[43moffset\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/datasets/arrow_dataset.py:3389\u001B[39m, in \u001B[36mDataset._map_single.<locals>.apply_function\u001B[39m\u001B[34m(pa_inputs, indices, offset)\u001B[39m\n\u001B[32m   3387\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001B[39;00m\n\u001B[32m   3388\u001B[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001B[32m-> \u001B[39m\u001B[32m3389\u001B[39m processed_inputs = \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfn_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43madditional_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfn_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3390\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 5\u001B[39m, in \u001B[36mtokenize\u001B[39m\u001B[34m(batch)\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtokenize\u001B[39m(batch):\n\u001B[32m      4\u001B[39m     \u001B[38;5;66;03m# TODO: Sequence packing\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m     outputs = \u001B[43mtokenizer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtext\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcontext_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     12\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[32m     13\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33minput_ids\u001B[39m\u001B[33m\"\u001B[39m: [\n\u001B[32m     14\u001B[39m             input_ids\n\u001B[32m   (...)\u001B[39m\u001B[32m     17\u001B[39m         ]\n\u001B[32m     18\u001B[39m     }\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:2887\u001B[39m, in \u001B[36mPreTrainedTokenizerBase.__call__\u001B[39m\u001B[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[39m\n\u001B[32m   2885\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._in_target_context_manager:\n\u001B[32m   2886\u001B[39m         \u001B[38;5;28mself\u001B[39m._switch_to_input_mode()\n\u001B[32m-> \u001B[39m\u001B[32m2887\u001B[39m     encodings = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_one\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mall_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2888\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m text_target \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   2889\u001B[39m     \u001B[38;5;28mself\u001B[39m._switch_to_target_mode()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:2975\u001B[39m, in \u001B[36mPreTrainedTokenizerBase._call_one\u001B[39m\u001B[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[39m\n\u001B[32m   2970\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   2971\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mbatch length of `text`: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(text)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m does not match batch length of `text_pair`:\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2972\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(text_pair)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2973\u001B[39m         )\n\u001B[32m   2974\u001B[39m     batch_text_or_text_pairs = \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(text, text_pair)) \u001B[38;5;28;01mif\u001B[39;00m text_pair \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m text\n\u001B[32m-> \u001B[39m\u001B[32m2975\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbatch_encode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2976\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2977\u001B[39m \u001B[43m        \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2978\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2979\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtruncation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2980\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2981\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstride\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2982\u001B[39m \u001B[43m        \u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2983\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2984\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2985\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2986\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2987\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2988\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2989\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2990\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2991\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2992\u001B[39m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2993\u001B[39m \u001B[43m        \u001B[49m\u001B[43msplit_special_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43msplit_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2994\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2995\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2996\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2997\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.encode_plus(\n\u001B[32m   2998\u001B[39m         text=text,\n\u001B[32m   2999\u001B[39m         text_pair=text_pair,\n\u001B[32m   (...)\u001B[39m\u001B[32m   3017\u001B[39m         **kwargs,\n\u001B[32m   3018\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:3177\u001B[39m, in \u001B[36mPreTrainedTokenizerBase.batch_encode_plus\u001B[39m\u001B[34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[39m\n\u001B[32m   3167\u001B[39m \u001B[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001B[39;00m\n\u001B[32m   3168\u001B[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001B[38;5;28mself\u001B[39m._get_padding_truncation_strategies(\n\u001B[32m   3169\u001B[39m     padding=padding,\n\u001B[32m   3170\u001B[39m     truncation=truncation,\n\u001B[32m   (...)\u001B[39m\u001B[32m   3174\u001B[39m     **kwargs,\n\u001B[32m   3175\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m3177\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_batch_encode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3178\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_text_or_text_pairs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3179\u001B[39m \u001B[43m    \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3180\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpadding_strategy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpadding_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3181\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtruncation_strategy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtruncation_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3182\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3183\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstride\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3184\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3185\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3186\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3187\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3188\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3189\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3190\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3191\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3192\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3193\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3194\u001B[39m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3195\u001B[39m \u001B[43m    \u001B[49m\u001B[43msplit_special_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43msplit_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3196\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3197\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py:116\u001B[39m, in \u001B[36mGPT2TokenizerFast._batch_encode_plus\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    110\u001B[39m is_split_into_words = kwargs.get(\u001B[33m\"\u001B[39m\u001B[33mis_split_into_words\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m    111\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.add_prefix_space \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_split_into_words, (\n\u001B[32m    112\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mYou need to instantiate \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m with add_prefix_space=True \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    113\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mto use it with pretokenized inputs.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    114\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_batch_encode_plus\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/transformers/tokenization_utils_fast.py:552\u001B[39m, in \u001B[36mPreTrainedTokenizerFast._batch_encode_plus\u001B[39m\u001B[34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001B[39m\n\u001B[32m    539\u001B[39m encodings = \u001B[38;5;28mself\u001B[39m._tokenizer.encode_batch(\n\u001B[32m    540\u001B[39m     batch_text_or_text_pairs,\n\u001B[32m    541\u001B[39m     add_special_tokens=add_special_tokens,\n\u001B[32m    542\u001B[39m     is_pretokenized=is_split_into_words,\n\u001B[32m    543\u001B[39m )\n\u001B[32m    545\u001B[39m \u001B[38;5;66;03m# Convert encoding to dict\u001B[39;00m\n\u001B[32m    546\u001B[39m \u001B[38;5;66;03m# `Tokens` has type: Tuple[\u001B[39;00m\n\u001B[32m    547\u001B[39m \u001B[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001B[39;00m\n\u001B[32m    548\u001B[39m \u001B[38;5;66;03m#                       List[EncodingFast]\u001B[39;00m\n\u001B[32m    549\u001B[39m \u001B[38;5;66;03m#                    ]\u001B[39;00m\n\u001B[32m    550\u001B[39m \u001B[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001B[39;00m\n\u001B[32m    551\u001B[39m tokens_and_encodings = [\n\u001B[32m--> \u001B[39m\u001B[32m552\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_convert_encoding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    553\u001B[39m \u001B[43m        \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    554\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    555\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    556\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    557\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    558\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    559\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    560\u001B[39m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    561\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    562\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m encoding \u001B[38;5;129;01min\u001B[39;00m encodings\n\u001B[32m    563\u001B[39m ]\n\u001B[32m    565\u001B[39m \u001B[38;5;66;03m# Convert the output to have dict[list] from list[dict] and remove the additional overflows dimension\u001B[39;00m\n\u001B[32m    566\u001B[39m \u001B[38;5;66;03m# From (variable) shape (batch, overflows, sequence length) to ~ (batch * overflows, sequence length)\u001B[39;00m\n\u001B[32m    567\u001B[39m \u001B[38;5;66;03m# (we say ~ because the number of overflow varies with the example in the batch)\u001B[39;00m\n\u001B[32m    568\u001B[39m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    569\u001B[39m \u001B[38;5;66;03m# To match each overflowing sample with the original sample in the batch\u001B[39;00m\n\u001B[32m    570\u001B[39m \u001B[38;5;66;03m# we add an overflow_to_sample_mapping array (see below)\u001B[39;00m\n\u001B[32m    571\u001B[39m sanitized_tokens = {}\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/transformers/tokenization_utils_fast.py:325\u001B[39m, in \u001B[36mPreTrainedTokenizerFast._convert_encoding\u001B[39m\u001B[34m(self, encoding, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001B[39m\n\u001B[32m    322\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    323\u001B[39m     encodings = [encoding]\n\u001B[32m--> \u001B[39m\u001B[32m325\u001B[39m encoding_dict = \u001B[43mdefaultdict\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    326\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m encodings:\n\u001B[32m    327\u001B[39m     encoding_dict[\u001B[33m\"\u001B[39m\u001B[33minput_ids\u001B[39m\u001B[33m\"\u001B[39m].append(e.ids)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "source": [
    "context_length = 20\n",
    "\n",
    "def tokenize(batch):\n",
    "    # TODO: Sequence packing\n",
    "    outputs = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": [\n",
    "            input_ids\n",
    "            for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"])\n",
    "            if length == context_length\n",
    "        ]\n",
    "    }\n",
    "\n",
    "tokenized_ds = dataset.map(\n",
    "    tokenize, batched=True, remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "tokenized_ds.save_to_disk(\"tokenized-wiki-ds.hf\")\n",
    "tokenized_ds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T20:22:24.177225Z",
     "start_time": "2025-06-06T20:22:15.255147Z"
    }
   },
   "id": "3245c8d35c4a038c"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 32, 512])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "seq_length = 32\n",
    "\n",
    "# TODO: Token and positional embedding\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, device=device)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "src = torch.rand(seq_length, 32, 512)\n",
    "causal_mask = nn.Transformer.generate_square_subsequent_mask(seq_length, device=device)\n",
    "transformer_encoder(src, mask=causal_mask).shape  # Skipping is_causal since seems troublesome: https://github.com/pytorch/pytorch/issues/96941\n",
    "# TODO: Add a linear layer to map the output to the vocabulary size, and then softmax on that"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-06T09:40:04.955726Z",
     "start_time": "2025-06-06T09:40:04.735779Z"
    }
   },
   "id": "8ddb9121087fad4e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f0ce3a4d861100b7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
