{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cpu')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "# Commented out because we yet again find mps to be drastically slower\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     torch._dynamo.disable()  # https://github.com/pytorch/pytorch/issues/149184\n",
    "#     device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"{device=}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-07T15:45:07.031529Z",
     "start_time": "2025-06-07T15:45:06.114087Z"
    }
   },
   "id": "f1f7c192645ed843"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-07T15:45:14.346036Z",
     "start_time": "2025-06-07T15:45:07.032209Z"
    }
   },
   "id": "dcc3bee03e0bf4e3"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    test: Dataset({\n        features: ['input_ids'],\n        num_rows: 12746\n    })\n    train: Dataset({\n        features: ['input_ids'],\n        num_rows: 5333343\n    })\n    validation: Dataset({\n        features: ['input_ids'],\n        num_rows: 11174\n    })\n})"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datasets import load_from_disk\n",
    "\n",
    "context_length = 20\n",
    "\n",
    "def tokenize(batch):\n",
    "    # TODO: Sequence packing\n",
    "    outputs = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": [\n",
    "            input_ids\n",
    "            for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"])\n",
    "            if length == context_length\n",
    "        ]\n",
    "    }\n",
    "\n",
    "if Path(\"tokenized-wiki-ds.hf\").exists():\n",
    "    tokenized_ds = load_from_disk(\"tokenized-wiki-ds.hf\")\n",
    "else:\n",
    "    tokenized_ds = dataset.map(\n",
    "        tokenize, batched=True, remove_columns=dataset[\"train\"].column_names\n",
    "    )\n",
    "    tokenized_ds.save_to_disk(\"tokenized-wiki-ds.hf\")\n",
    "tokenized_ds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-07T15:46:20.691819Z",
     "start_time": "2025-06-07T15:46:19.791243Z"
    }
   },
   "id": "3245c8d35c4a038c"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacob/repos/reimplementing/.pixi/envs/default/lib/python3.13/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "[' Theft', ' playable']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim, device):\n",
    "        super().__init__()\n",
    "        self.numerators = 10_000 ** (  # TODO: Why 10_000?\n",
    "            torch.arange(\n",
    "                start=0,\n",
    "                end=embedding_dim,\n",
    "                step=2,\n",
    "                device=device,\n",
    "            ).float()\n",
    "            / embedding_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor):\n",
    "        with torch.no_grad():\n",
    "            positions = torch.arange(\n",
    "                input_ids.shape[1],\n",
    "                device=input_ids.device,\n",
    "            ).float()\n",
    "            raw_embeddings = positions.unsqueeze(1) @ (1 / self.numerators).unsqueeze(0)\n",
    "            even_embeddings = torch.sin(raw_embeddings)\n",
    "            odd_embeddings = torch.cos(raw_embeddings)\n",
    "            embeddings = torch.stack(\n",
    "                [even_embeddings, odd_embeddings], dim=-1\n",
    "            ).view(\n",
    "                len(positions), -1\n",
    "            )\n",
    "            return embeddings.unsqueeze(0).expand(input_ids.shape[0], -1, -1)\n",
    "\n",
    "\n",
    "token_embedder = nn.Embedding(\n",
    "    num_embeddings=tokenizer.vocab_size, embedding_dim=512, device=device\n",
    ")\n",
    "positional_embedder = PositionalEmbedding(embedding_dim=512, device=device)\n",
    "transformer_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, device=device)\n",
    "transformer = nn.TransformerEncoder(transformer_layer, num_layers=6)\n",
    "decoder = nn.Linear(512, tokenizer.vocab_size, device=device)\n",
    "\n",
    "src = [\"Hi, my name\", \"The United States of\"]\n",
    "tokenized = tokenizer(src, return_tensors=\"pt\").to(device)\n",
    "embedded = token_embedder(tokenized.input_ids) + positional_embedder(tokenized.input_ids)\n",
    "transformed = transformer(\n",
    "    embedded.permute(1, 0, 2),  # Transformer expects (seq_len, batch_size, features)\n",
    "    mask=nn.Transformer.generate_square_subsequent_mask(tokenized.input_ids.shape[1], device=device),\n",
    "    # Skipping is_causal since seems troublesome: https://github.com/pytorch/pytorch/issues/96941\n",
    ")\n",
    "logits = decoder(transformed.permute(1, 0, 2))  # Back to (batch_size, seq_len, features)\n",
    "result = tokenizer.batch_decode(logits[:, -1, :].argmax(dim=-1))\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-07T15:46:21.126936Z",
     "start_time": "2025-06-07T15:46:20.690283Z"
    }
   },
   "id": "8ddb9121087fad4e"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "MyGPT(\n  (token_embedder): Embedding(50257, 512)\n  (positional_embedder): PositionalEmbedding()\n  (transformer): TransformerEncoder(\n    (layers): ModuleList(\n      (0-5): 6 x TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (decoder): Linear(in_features=512, out_features=50257, bias=True)\n)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyGPT(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.token_embedder = nn.Embedding(\n",
    "            num_embeddings=tokenizer.vocab_size, embedding_dim=d_model, device=device\n",
    "        )\n",
    "        self.positional_embedder = PositionalEmbedding(embedding_dim=d_model, device=device)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, device=device),\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.decoder = nn.Linear(d_model, tokenizer.vocab_size, device=device)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor):\n",
    "        embedded = self.token_embedder(input_ids) + self.positional_embedder(input_ids)\n",
    "        transformed = self.transformer(\n",
    "            embedded.permute(1, 0, 2),  # Transformer expects (seq_len, batch_size, features)\n",
    "            mask=nn.Transformer.generate_square_subsequent_mask(input_ids.shape[1], device=input_ids.device),\n",
    "        )\n",
    "        logits = self.decoder(transformed.permute(1, 0, 2))\n",
    "        return logits\n",
    "    \n",
    "    def stream(self, input_ids: torch.Tensor, max_length=50):\n",
    "        # TODO: KV-cache to avoid quadratic computational complexity in `max_length`\n",
    "        output_ids = input_ids.clone()\n",
    "        for _ in range(max_length):\n",
    "            with torch.no_grad():\n",
    "                logits = self(output_ids)\n",
    "                # TODO: Support stochastic sampling\n",
    "                next_token_id = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "                output_ids = torch.cat([output_ids, next_token_id], dim=1)\n",
    "                yield next_token_id.item()\n",
    "    \n",
    "    def print_stream(self, tokenizer, prompt: str, max_length=50):\n",
    "        print(prompt, end=\"\", flush=True)\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "        for token in self.stream(input_ids, max_length=max_length):\n",
    "            if token == tokenizer.eos_token_id:\n",
    "                break\n",
    "            print(tokenizer.decode(token), end=\"\", flush=True)\n",
    "\n",
    "\n",
    "model = MyGPT(d_model=512, nhead=8, num_layers=6, device=device)\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-07T16:02:34.969769Z",
     "start_time": "2025-06-07T16:02:34.611254Z"
    }
   },
   "id": "f0ce3a4d861100b7"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "[' affordable', ' Harden']"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(model(tokenizer(src, return_tensors=\"pt\").input_ids.to(device))[:, -1, :].argmax(dim=-1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-07T16:02:37.680802Z",
     "start_time": "2025-06-07T16:02:37.652498Z"
    }
   },
   "id": "96d84dc850df0508"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States of cruise Plan777 inducing cruise Plan LS cruise Plan LS cruise Plan LS 13 clutching brandeddecl Contrast LS 13 LS 13 LS�� LS 13 LS 13 asleep Casribut Suppose� quicker Polyribut ``��elfth drinkersryptedimmigrant 106Bot LS��Chapter branded 106Bot\n",
      "The United States of cruise Plan cruise Plan LS deceased Corps deceased Corps cruiseAverage LIM 0000 clutchingRy LS 13 LS 13 LS�� LS 13 robbed assortment LS 13 entails................................................................ribut giveaway LS��catchACPChapter branded 106Botribut simul LSyre LS��Chapterribut Suppose 106 106\n",
      "The United States of Lakes bigger cruise moth cer cruise Plan LS cruise Plan LS 13 0000 106 designsyre LS 13 LS 13nesia LS 13zunesia LS�� LS 13 giveaway LS 13 giveawayribut Suppose 106 simul 106Bot 106Bot 13 simul branded simul 13 asleep === 13 simul\n",
      "The United States of stay cruise damaged disposed damaged Ninth cruise Plan LS cruise damaged quicker PolyguardsRy 106Ry LS cruiseyre LS `` 106 daemonRy LS Cas ``IFIED...... 106Bot ``akespeare clutchingRy 106 sus kernel 106Bot 106Bot 106 daemonRy 106 106Bot 106\n",
      "The United States of stay cruise PlanChapter Yoga cruise012 CK cruise affordableslice LS cruise cruise levels LS 13ACPChapter brandedzar Mercy trash 106 designs quicker iOS 106Victoria 106Bot 106 stocking Cas quicker adventure 106 designs quicker reconsider clutchingRy 106 designs strategy Love `` 106も Slovakia\n",
      "The United States of cruise Plan LS cruise Plan LS cruise Silicon cruise Plan LS cruiseAverage Marcus LS 13 LS 13 LSyre LS 13 coaching�� LS�� LS 13inguishableGs LS 13 LS 13�� Sed 13��elfth unarmed analogue LS 13impact inducing regain giveaway�� LS��\n",
      "The United States of Lakes cruise PlanChapter cruise clutching cruise cruise cruise prefix LS cruiseBot ticketChapter branded cruisedecl CK LS Casparen LS cruise clutching 0000 CasadjustVictoria LS Casadjust LS 13 clutching 106Bot 106Bot 106Bot 106 designs clutchingRy clutchingRy fetus 106Bot\n",
      "The United States of cruise moth fianceIntroduction 269 disposed Marcus 0000Ry LS cruise Siliconobby Marcus hover Art clutching 106Victoria LS 13 0000 Bone LS 13 LS 13 LS 13 clutchingRy 106Bot 106 designsyre Silicon LIMAverage quicker quicker reconsider clutchingRy 106Bot 106Bot 106akespeare\n",
      "The United States of cruise mothbda cruise Cummings possessedINAL cruise cruise iOSnesia Les cruiseilar clutching cruise branded brandedzar hoverzar trashVictoria LS LS LS 13 asleep LS 13 clutchingRy LS 13 clutching branded exclusively upright 13 asleep Korea clutching 106 designsyre LS�� Yoga 13Introduction\n",
      "The United States of stay stay cruise PlanChapter stayzarlo damaged quickerTokennesiaChapter branded cruise damaged quicker Paladin CKoti `` EarthquACP012................................................................Token 106 daemonvalid012 CK012 CK clutchingRy 106も� Contrast 106も� Pence Polyribut simul branded exclusively upright con\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    model.print_stream(tokenizer=tokenizer, prompt=\"The United States of\", max_length=50)\n",
    "    print(\"\", flush=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-07T16:04:21.035787Z",
     "start_time": "2025-06-07T16:04:10.054350Z"
    }
   },
   "id": "4ace6d0854424cb8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6e4c6dfba7ad523"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
